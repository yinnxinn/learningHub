---
title: "The Illustrated GPT-2"
description: "图解 GPT-2 的自回归语言模型结构，并展示文本生成的工作流程。"
url: "https://jalammar.github.io/illustrated-gpt2/"
tags: ["大模型", "GPT", "语言模型"]
contributor: "@learning-share"
recommendation: 4
---

## 推荐理由
- 对自回归 Transformer 的输入输出流程进行直观展示。
- 解释词表、上下文窗口、采样策略等关键概念。
- 提供生成案例与超参数说明，为调优提供参考。

## 使用建议
1. 结合 OpenAI 的 GPT-2 代码或 Hugging Face Demo，复现文本生成实验。
2. 尝试修改采样策略（Top-k、Top-p），观察输出差异。
3. 将 GPT-2 与 BERT、GPT-3 的架构对比，理解编码器-解码器与纯解码器的差异。
